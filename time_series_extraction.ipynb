{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json as js\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import utils as u\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fields of Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Preparing Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Extracting FieldsOfStudy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -I pixz -xvf data/FieldsOfStudy.tar.xz --directory data/\n",
    "#! tar -xvf data/FieldsOfStudy.tar.xz --directory data/\n",
    "! rm data/FieldsOfStudy.tar.xz\n",
    "raw_fields = pd.read_table(\"data/FieldsOfStudy.txt\")[[\"FieldOfStudyId\", \"NormalizedName\", \"Level\"]]\n",
    "raw_fields['NormalizedName'].fillna(\"na\", inplace = True)\n",
    "fields = raw_fields[raw_fields[\"Level\"] == 0][[\"FieldOfStudyId\", \"NormalizedName\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Extracting Subfield Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! tar -I pixz -xvf data/FieldOfStudyChildren.tar.xz --directory data/\n",
    "! tar -xvf data/FieldOfStudyChildren.tar.xz --directory data/\n",
    "! rm data/FieldOfStudyChildren.tar.xz\n",
    "children = pd.read_table(\"data/FieldOfStudyChildren.txt\")\n",
    "\n",
    "subfields = []\n",
    "for id in fields[\"FieldOfStudyId\"]:\n",
    "    subs = np.empty(0, dtype=np.int64)\n",
    "    curr = np.array([id], dtype = np.int64)\n",
    "    while curr.size != 0:\n",
    "        tmp = children[children[\"FieldOfStudyId\"].isin(curr)][\"ChildFieldOfStudyId\"]\n",
    "        subs = np.append(subs, curr)\n",
    "        curr = tmp\n",
    "    subfields.append(subs)\n",
    "fields[\"Subfields\"] = np.array(subfields)\n",
    "fields.columns = ['FieldId', 'Field', 'Subfields']\n",
    "\n",
    "# The \"computational\" fields will be needed in the future\n",
    "cs_subfields = fields[fields[\"Field\"] == \"computer science\"][\"Subfields\"].values[0]\n",
    "computational = raw_fields[raw_fields['NormalizedName'].str.contains(\"comp( |ut)\")]\n",
    "computational = computational[~computational['NormalizedName'].str.contains(\"tomograph\")]  # CT scans happen\n",
    "computational = [f for f in computational['FieldOfStudyId'] if f not in cs_subfields ]\n",
    "fields = fields.append(\n",
    "    {\"FieldId\":-1, \"Field\":\"computational\", \"Subfields\":np.array(computational)}, \n",
    "    ignore_index=True)\n",
    "\n",
    "fields.reset_index(drop=True).to_feather(\"data/Fields\")\n",
    "! rm data/FieldsOfStudy.txt\n",
    "! rm data/FieldOfStudyChildren.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Reducing PaperFieldsOfStudy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Extracting PaperFieldsOfStudy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! tar  -I pixz -xvf data/PaperFieldsOfStudy.tar.xz --directory data/\n",
    "! tar -xvf data/PaperFieldsOfStudy.tar.xz --directory data/\n",
    "! rm data/PaperFieldsOfStudy.tar.xz\n",
    "! mkdir data/PaperFieldsOfStudy\n",
    "! split -C 2G data/PaperFieldsOfStudy.txt data/PaperFieldsOfStudy/PaperFieldsOfStudy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the strength of the associations\n",
    "schema = u.load_schema(\"PaperFieldsOfStudy\")\n",
    "scores = np.empty(0, dtype = np.float16)\n",
    "\n",
    "for part in os.listdir(\"data/PaperFieldsOfStudy\"):\n",
    "    part_scores = pd.read_table(\"data/PaperFieldsOfStudy/\"+part, names = schema)[\"Score\"].values.astype('float16')\n",
    "    scores = np.append(scores, part_scores)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(scores, bins=40)\n",
    "ax.set_xlabel(\"Field Score\")\n",
    "ax.set_ylabel(\"Number of Associations\\n(100 millions)\")\n",
    "fig.savefig(\"plots/FieldScoresDistribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Reducing Fields to the Top Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subf_papers = set([])\n",
    "comp_papers = set([])\n",
    "fields = pd.read_feather(\"data/Fields\")\n",
    "comp_fields = fields[fields[\"Field\"] == \"computational\"][\"Subfields\"].values[0]\n",
    "fields = fields[fields[\"Field\"] != \"computational\"]\n",
    "top_dict = {key:fields[fields[\"FieldId\"] == key][\"Subfields\"].values[0] for key in fields[\"FieldId\"].values}\n",
    "cs_id = fields[fields[\"Field\"] == \"computer science\"][\"FieldId\"].values[0]\n",
    "schema = u.load_schema(\"PaperFieldsOfStudy\")\n",
    "schema[1] = \"FieldId\"\n",
    "\n",
    "\n",
    "for part in os.listdir(\"data/PaperFieldsOfStudy\"):\n",
    "    tmp = pd.read_table(\"data/PaperFieldsOfStudy/\"+part, names = schema)\n",
    "    # Eliminate score outliers, i.e. associations with too low scores\n",
    "    tmp = tmp[tmp[\"Score\"] > .05][[\"FieldId\", \"PaperId\"]]\n",
    "    # Set aside these target papers\n",
    "    comp_papers.update(tmp[tmp['FieldId'].isin(comp_fields)]['PaperId'].unique().tolist())\n",
    "    # Convert all fields to the top field, taking care of intersections\n",
    "    current = []\n",
    "    for top in top_dict.keys():\n",
    "        papers = tmp[tmp[\"FieldId\"].isin(top_dict[top])][\"PaperId\"].unique()\n",
    "        current.append(pd.DataFrame({\"PaperId\":papers,\"FieldId\":top}))\n",
    "        # Set aside these target papers\n",
    "        if top == cs_id:\n",
    "            subf_papers.update(papers.tolist())\n",
    "    pd.concat(current).reset_index(drop=True).to_feather(\"data/PaperFieldsOfStudy/\"+part)\n",
    "\n",
    "conditions = [\"CS\"]*len(subf_papers) + [\"comp\"]*len(comp_papers)\n",
    "papers = list(subf_papers) + list(comp_papers)\n",
    "pd.DataFrame({\"PaperId\":np.array(papers), \"Condition\":conditions}).to_feather(\"data/CitedPapers\")\n",
    "\n",
    "! rm data/PaperFieldsOfStudySchema.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Splitting by Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = pd.read_feather(\"data/Fields\")\n",
    "parts = os.listdir(\"data/PaperFieldsOfStudy\")\n",
    "for field in fields[\"FieldId\"].unique()[:-1]:\n",
    "    fi_papers = []\n",
    "    for part in parts:\n",
    "        tmp = pd.read_feather(\"data/PaperFieldsOfStudy/\"+part)\n",
    "        fi_papers.append(tmp[tmp[\"FieldId\"] == field][[\"PaperId\"]])\n",
    "    tmp = pd.concat(fi_papers).drop_duplicates()\n",
    "    tmp.reset_index(drop = True).to_feather(\"data/PaperFieldsOfStudy/\"+str(field))\n",
    "\n",
    "! rm data/PaperFieldsOfStudy/Paper*\n",
    "! mv data/PaperFieldsOfStudy/ data/PaperFields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Extracting Paper Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -xvf data/Papers.tar.xz --directory data/\n",
    "#! tar -I pixz -xvf data/Papers.tar.xz --directory data/\n",
    "! rm data/Papers.tar.xz\n",
    "! mkdir data/Papers\n",
    "! split -C 3G data/Papers.txt data/Papers/Papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Pruning Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = u.load_schema(\"Papers\")\n",
    "\n",
    "for part in os.listdir(\"data/Papers\"):\n",
    "    tmp = pd.read_table(\"data/Papers/\"+part, names = schema, \n",
    "        usecols = [\"PaperId\", \"PaperTitle\", \"Date\", \"OnlineDate\"])\n",
    "    tmp.drop_duplicates([\"PaperTitle\", \"Date\"], inplace=True)\n",
    "    tmp[[\"PaperId\", \"Date\", \"OnlineDate\"]].reset_index(drop=True).to_feather(\"data/Papers/\"+part)\n",
    "\n",
    "# NOTE: duplicates appear\n",
    "# NOTE: DocSubTypes is not in the dataset; it needs to be taken out from PapersSchema.txt\n",
    "! rm data/PapersSchema.txt\n",
    "! rm data/Papers.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Fixing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_date = 0\n",
    "for part in os.listdir(\"data/Papers\"):\n",
    "    tmp = pd.read_feather(\"data/Papers/\"+part)\n",
    "    tmp[\"Date\"] = pd.to_datetime(tmp[\"Date\"])\n",
    "    tmp[tmp[\"Date\"].notna()][[\"PaperId\", \"Date\"]].reset_index(drop=True).to_feather(\"data/Papers/\"+part)\n",
    "    no_date += len(tmp[tmp[\"Date\"].isna()])\n",
    "\n",
    "print(no_date) # 6679"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PaperReferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Extracting PaperReferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -xvf data/PaperReferences.tar.xz --directory data/\n",
    "#! tar -I pixz -xvf data/PaperReferences.tar.xz --directory data/\n",
    "! rm data/PaperReferences.tar.xz\n",
    "! mkdir data/PaperReferences\n",
    "! split -C 2G data/PaperReferences.txt data/PaperReferences/PaperReferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize\n",
    "schema = u.load_schema(\"PaperReferences\")\n",
    "\n",
    "for part in os.listdir(\"data/PaperReferences\"):\n",
    "    tmp = pd.read_table(\"data/PaperReferences/\"+part, names = schema)\n",
    "    tmp.to_feather(\"data/PaperReferences/\"+part)\n",
    "\n",
    "! rm data/PaperReferencesSchema.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Extracting References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data/References\n",
    "\n",
    "fields = pd.read_feather(\"data/Fields\")[\"FieldId\"].unique()[:-1]\n",
    "parts = os.listdir(\"data/PaperReferences\")\n",
    "for field in fields:\n",
    "    papers = pd.read_feather(\"data/PaperFields/\"+str(field))[\"PaperId\"].values\n",
    "    refs = []\n",
    "    for part in parts:\n",
    "        tmp = pd.read_feather(\"data/PaperReferences/\"+part)\n",
    "        tmp = tmp[tmp[\"PaperId\"].isin(papers)][[\"PaperId\"]]\n",
    "        tmp[\"References\"] = 1\n",
    "        refs.append(tmp.groupby(\"PaperId\").sum().reset_index()) # Required for memory saving\n",
    "    refs = pd.concat(refs)\n",
    "    refs = refs.groupby(\"PaperId\").sum().reset_index()\n",
    "    refs.to_feather(\"data/References/\"+str(field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With low processing, power, the following can be performed \"in log form\" separately (i.e. recursively 2 by 2)\n",
    "overall = []\n",
    "for part in os.listdir(\"data/References\"):\n",
    "    overall.append(pd.read_feather(\"data/References/\"+part))\n",
    "pd.concat(overall).drop_duplicates().reset_index().to_feather(\"data/References/0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Extracting Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. Pruning Unneeded References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.read_feather(\"data/CitedPapers\")[\"PaperId\"].unique()\n",
    "no_paper_data = 0\n",
    "for part in os.listdir(\"data/PaperReferences\"):\n",
    "    tmp = pd.read_feather(\"data/PaperReferences/\"+part)\n",
    "    pre = len(tmp)\n",
    "    tmp = tmp[tmp[\"PaperReferenceId\"].isin(papers)]\n",
    "    no_paper_data += (pre - len(tmp))\n",
    "    tmp.reset_index(drop=True).to_feather(\"data/PaperReferences/\"+part)\n",
    "\n",
    "# The references contained a number of papers\n",
    "# for which no data was available in the Papers file\n",
    "print(no_paper_data) # 1259719004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Splitting in Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data/Citations\n",
    "\n",
    "fields = pd.read_feather(\"data/Fields\")[\"FieldId\"].unique()[:-1]\n",
    "tmp = pd.read_feather(\"data/CitedPapers\")\n",
    "cs = tmp[tmp[\"Condition\"] == \"CS\"][\"PaperId\"].values\n",
    "comp = tmp[tmp[\"Condition\"] == \"comp\"][\"PaperId\"].values\n",
    "\n",
    "parts = os.listdir(\"data/PaperReferences\")\n",
    "for field in fields:\n",
    "    papers = pd.read_feather(\"data/PaperFields/\"+str(field))[\"PaperId\"].values\n",
    "    cs_cits, comp_cits = [], []\n",
    "\n",
    "    for part in parts:\n",
    "        tmp = pd.read_feather(\"data/PaperReferences/\"+part)\n",
    "        tmp = tmp[tmp[\"PaperId\"].isin(papers)]\n",
    "        # CS condition\n",
    "        csc = tmp[tmp[\"PaperReferenceId\"].isin(cs)][[\"PaperId\"]]\n",
    "        csc[\"Citations\"] = 1\n",
    "        cs_cits.append(csc.groupby(\"PaperId\").sum().reset_index()) # Required for memory saving\n",
    "        # comp condition\n",
    "        compc = tmp[tmp[\"PaperReferenceId\"].isin(comp)][[\"PaperId\"]]\n",
    "        compc[\"Citations\"] = 1\n",
    "        comp_cits.append(compc.groupby(\"PaperId\").sum().reset_index()) # Required for memory saving\n",
    "        # all condition\n",
    "        tmp = tmp[[\"PaperId\"]]\n",
    "        tmp[\"Citations\"] = 1\n",
    "    \n",
    "    cs_cits = pd.concat(cs_cits).groupby(\"PaperId\").sum().reset_index()\n",
    "    cs_cits.to_feather(\"data/Citations/\"+str(field)+\"_CS\")\n",
    "    comp_cits = pd.concat(comp_cits).groupby(\"PaperId\").sum().reset_index()\n",
    "    comp_cits.to_feather(\"data/Citations/\"+str(field)+\"_Comp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With low processing, power, the following can be performed \"in log form\" separately (i.e. recursively 2 by 2)\n",
    "overall = []\n",
    "for field in fields:\n",
    "    overall.append(pd.read_feather(\"data/Citations/\"+str(field)+\"_CS\"))\n",
    "pd.concat(overall).drop_duplicates().reset_index().to_feather(\"data/Citations/0_CS\")\n",
    "\n",
    "overall = []\n",
    "for field in fields:\n",
    "    overall.append(pd.read_feather(\"data/Citations/\"+str(field)+\"_Comp\"))\n",
    "pd.concat(overall).drop_duplicates().reset_index().to_feather(\"data/Citations/0_Comp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. Outliers and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = pd.read_feather(\"data/References/0\")\n",
    "out_no = len(overall)\n",
    "overall = overall[overall[\"References\"] < 301][\"References\"].values\n",
    "out_no -= len(overall)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(overall, bins=100)\n",
    "ax.set_xlabel(\"Number of References\")\n",
    "ax.set_ylabel(\"Number of Papers\\n(10 millions)\")\n",
    "fig.savefig(\"images/ReferenceNumberDistribution300.png\")\n",
    "\n",
    "# There are some outliers that are\n",
    "# recorded having more than 300 references:\n",
    "print(out_no) # 106695\n",
    "# Remaining:\n",
    "print(len(overall)) # 82961477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = pd.concat(\n",
    "    pd.read_feather(\"data/Citations/0_CS\")[[\"Citations\"]],\n",
    "    pd.read_feather(\"data/Citations/0_Comp\")[[\"Citations\"]])\n",
    "out_no = len(overall)\n",
    "overall = overall[overall[\"Citations\"] < 1001][\"Citations\"].unique().values\n",
    "out_no -= len(overall)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(overall, bins=100)\n",
    "ax.set_xlabel(\"Number of Citations\")\n",
    "ax.set_ylabel(\"Number of Papers\\n(10 millions)\")\n",
    "fig.savefig(\"images/CitationsNumberDistribution1000.png\")\n",
    "\n",
    "# There are some outliers that are \n",
    "# recorded being cited more than 1000 times:\n",
    "print(out_no) # 1805\n",
    "# Remaining:\n",
    "print(len(overall)) # 64130026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in os.listdir(\"data/References\"):\n",
    "    tmp = pd.read_feather(\"data/References/\"+part)\n",
    "    tmp = tmp[tmp[\"References\"] < 301]\n",
    "    tmp.reset_index(drop = True).to_feather(\"data/References/\"+part)\n",
    "\n",
    "for part in os.listdir(\"data/Citations\"):\n",
    "    tmp = pd.read_feather(\"data/Citations/\"+part)\n",
    "    tmp = tmp[tmp[\"Citations\"] < 1001]\n",
    "    tmp.reset_index(drop = True).to_feather(\"data/Citations/\"+part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Preparing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data/ReferencesDates \n",
    "\n",
    "papers = pd.read_feather(\"data/References/0\")[\"PaperId\"].unique()\n",
    "\n",
    "for part in os.listdir(\"data/Papers\"):\n",
    "    tmp = pd.read_feather(\"data/Papers/\"+part)\n",
    "    tmp = tmp[tmp[\"PaperId\"].isin(papers)]\n",
    "    tmp[\"Date\"] = tmp[\"Date\"].dt.to_period(freq = \"M\")\n",
    "    tmp.reset_index(drop = True).to_feather(\"data/ReferencesDates/\"+part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data/CitationsDates \n",
    "\n",
    "papers = pd.concat(\n",
    "    pd.read_feather(\"data/Citations/0_CS\")[[\"PaperId\"]],\n",
    "    pd.read_feather(\"data/Citations/0_Comp\")[[\"PaperId\"]])[\"PaperId\"].unique()\n",
    "\n",
    "for part in os.listdir(\"data/Papers\"):\n",
    "    tmp = pd.read_feather(\"data/Papers/\"+part)\n",
    "    tmp = tmp[tmp[\"PaperId\"].isin(papers)]\n",
    "    tmp[\"Date\"] = tmp[\"Date\"].dt.to_period(freq = \"M\")\n",
    "    tmp.reset_index(drop = True).to_feather(\"data/CitationsDates/\"+part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Adding Time Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "for part in os.listdir(\"data/ReferencesDates\"):\n",
    "    dates.append(pd.read_feather(\"data/ReferencesDates/\"+part))\n",
    "dates = pd.concat(dates)\n",
    "\n",
    "for part in os.listdir(\"data/References\"):\n",
    "    tmp = pd.read_feather(\"data/References/\"+part)\n",
    "    tmp = tmp.merge(dates, on = \"PaperId\", how = \"left\")[[\"Date\", \"References\"]]\n",
    "    tmp = tmp.dropna()\n",
    "    tmp = tmp.groupby(\"Date\").sum().reset_index()\n",
    "    tmp.to_feather(\"data/References/\"+part)\n",
    "\n",
    "! rm -rf data/ReferencesDates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "for part in os.listdir(\"data/CitationsDates\"):\n",
    "    dates.append(pd.read_feather(\"data/CitationsDates/\"+part))\n",
    "dates = pd.concat(dates)\n",
    "\n",
    "for part in os.listdir(\"data/Citations\"):\n",
    "    tmp = pd.read_feather(\"data/Citations/\"+part)\n",
    "    tmp = tmp.merge(dates, on = \"PaperId\", how = \"left\")[[\"Date\", \"Citations\"]]\n",
    "    tmp = tmp.dropna()\n",
    "    tmp = tmp.groupby(\"Date\").sum().reset_index()\n",
    "    tmp.to_feather(\"data/Citations/\"+part)\n",
    "\n",
    "! rm -rf data/CitationsDates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Preparing Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Restricting Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in os.listdir(\"data/Citations\"):\n",
    "    tmp = pd.read_feather(\"data/Citations/\"+part)\n",
    "    tmp = tmp[tmp[\"Date\"].dt.year > 1959]\n",
    "    tmp = tmp[tmp[\"Date\"].dt.year < 2021]\n",
    "    tmp.reset_index(drop = True).to_feather(\"data/Citations/\"+part)\n",
    "\n",
    "for part in os.listdir(\"data/References\"):\n",
    "    tmp = pd.read_feather(\"data/References/\"+part)\n",
    "    tmp = tmp[tmp[\"Date\"].dt.year > 1959]\n",
    "    tmp = tmp[tmp[\"Date\"].dt.year < 2021]\n",
    "    tmp.reset_index(drop = True).to_feather(\"data/References/\"+part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Fill in Unavailable Time Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for part in os.listdir(\"data/Citations\"):\n",
    "    tmp = pd.read_feather(\"data/Citations/\"+part)\n",
    "    if len(tmp) != 732:\n",
    "        print(part)\n",
    "    lens.append(len(tmp))\n",
    "print(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_name = os.listdir(\"data/Citations\")[27]\n",
    "fix = pd.read_feather(\"data/Citations/\"+fix_name)\n",
    "full = os.listdir(\"data/Citations\")[0]\n",
    "full = pd.read_feather(\"data/Citations/\"+full)\n",
    "full = full[~full[\"Date\"].isin(fix[\"Date\"])][\"Date\"].values[0]\n",
    "fix = pd.concat([fix, pd.DataFrame({\"Date\":[full], \"Citations\":[0]})])\n",
    "fix.reset_index(drop=True).to_feather(\"data/Citations/\"+fix_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for part in os.listdir(\"data/References\"):\n",
    "    tmp = pd.read_feather(\"data/References/\"+part)\n",
    "    lens.append(len(tmp))\n",
    "print(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3. Adjust Citations by References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for part in os.listdir(\"data/References\"):\n",
    "    refs = pd.read_feather(\"data/References/\"+part).sort_values(by=\"Date\")[\"References\"]\n",
    "    tmp = pd.read_feather(\"data/Citations/\"+part+\"_CS\").sort_values(by=\"Date\")\n",
    "    tmp[\"AdjCitations\"] = tmp[\"Citations\"]/refs\n",
    "    tmp.reset_index(drop = True).to_feather(\"data/Citations/\"+part+\"_CS\")\n",
    "    tmp = pd.read_feather(\"data/Citations/\"+part+\"_Comp\").sort_values(by=\"Date\")\n",
    "    tmp[\"AdjCitations\"] = tmp[\"Citations\"]/refs\n",
    "    tmp.reset_index(drop = True).to_feather(\"data/Citations/\"+part+\"_Comp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4. Compress in Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = []\n",
    "fields = pd.read_feather(\"data/Fields\")[[\"FieldId\", \"Field\"]]\n",
    "fields = pd.concat([fields, pd.DataFrame({\"FieldId\":[0], \"Field\":[\"overall\"]})])\n",
    "for part in os.listdir(\"data/Citations\"):\n",
    "    tmp = pd.read_feather(\"data/Citations/\"+part)\n",
    "    field, condition = part.split(\"_\")\n",
    "    tmp[\"Field\"] = fields[fields[\"FieldId\"] == int(field)][\"Field\"].values[0]\n",
    "    tmp[\"Condition\"] = condition\n",
    "    citations.append(tmp)\n",
    "pd.concat(citations).to_csv(\"data/CitationsTS\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = []\n",
    "fields = pd.read_feather(\"data/Fields\")[[\"FieldId\", \"Field\"]]\n",
    "fields = pd.concat([fields, pd.DataFrame({\"FieldId\":[0], \"Field\":[\"overall\"]})])\n",
    "for part in os.listdir(\"data/References\"):\n",
    "    tmp = pd.read_feather(\"data/References/\"+part)\n",
    "    tmp[\"Field\"] = fields[fields[\"FieldId\"] == int(part)][\"Field\"].values[0]\n",
    "    references.append(tmp)\n",
    "pd.concat(references).to_csv(\"data/ReferencesTS\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Final Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Visual Confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_csv(\"data/CitationsTS\")\n",
    "\n",
    "for cond in ts[\"Condition\"].unique():\n",
    "    fig = plt.figure(figsize = (21, 7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    for field in ts[\"Field\"].unique():\n",
    "        tmp = ts[(ts[\"Condition\"] == cond) & (ts[\"Field\"] == field)][[\"Date\", \"AdjCitations\"]]\n",
    "        tmp[\"Date\"] = pd.to_datetime(tmp[\"Date\"]).dt.to_period(freq=\"M\")\n",
    "        tmp = tmp.set_index(\"Date\")\n",
    "        ax.plot(tmp.to_timestamp(freq = \"M\"), label = field)\n",
    "    ax.legend()\n",
    "    ax.set_title(cond)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_csv(\"data/CitationsTS\")\n",
    "\n",
    "for cond in ts[\"Condition\"].unique():\n",
    "    fig = plt.figure(figsize = (21, 7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    for field in ts[\"Field\"].unique():\n",
    "        tmp = ts[(ts[\"Condition\"] == cond) & (ts[\"Field\"] == field)][[\"Date\", \"Citations\"]]\n",
    "        tmp[\"Date\"] = pd.to_datetime(tmp[\"Date\"]).dt.to_period(freq=\"M\")\n",
    "        tmp = tmp.set_index(\"Date\")\n",
    "        ax.plot(tmp.to_timestamp(freq = \"M\"), label = field)\n",
    "    ax.legend()\n",
    "    ax.set_title(cond)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_csv(\"data/ReferencesTS\")\n",
    "\n",
    "fig = plt.figure(figsize = (21, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "for field in ts[\"Field\"].unique():\n",
    "    tmp = ts[ts[\"Field\"] == field][[\"Date\", \"References\"]]\n",
    "    tmp[\"Date\"] = pd.to_datetime(tmp[\"Date\"]).dt.to_period(freq=\"M\")\n",
    "    tmp = tmp.set_index(\"Date\")\n",
    "    ax.plot(tmp.to_timestamp(freq = \"M\"), label = field)\n",
    "ax.legend()\n",
    "ax.set_title(cond)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_csv(\"data/CitationsTS\")\n",
    "ts[\"Date\"] = pd.to_datetime(ts[\"Date\"]).dt.to_period(freq=\"M\")\n",
    "        \n",
    "\n",
    "metrics = ts[[\"Field\", \"Condition\", \"AdjCitations\"]].groupby([\"Field\", \"Condition\"]).mean().reset_index()\n",
    "metrics[\"Citations\"] = ts[[\"Field\", \"Condition\", \"Citations\"]].groupby([\"Field\", \"Condition\"]).sum().reset_index()[\"Citations\"]\n",
    "ts = ts[ts[\"Date\"].dt.year > 1999]\n",
    "metrics[\"AdjCitations_2000\"] = ts[[\"Field\", \"Condition\", \"AdjCitations\"]].groupby([\"Field\", \"Condition\"]).mean().reset_index()[\"AdjCitations\"]\n",
    "metrics[\"Citations_2000\"] = ts[[\"Field\", \"Condition\", \"Citations\"]].groupby([\"Field\", \"Condition\"]).sum().reset_index()[\"Citations\"]\n",
    "metrics.sort_values(by = \"Condition\").to_csv(\"data/Metrics\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf data/Citations\n",
    "! rm -rf data/CitationsDates\n",
    "! rm -rf data/PaperFields\n",
    "! rm -rf data/PaperReferences\n",
    "! rm -rf data/Papers\n",
    "! rm -rf data/References\n",
    "! rm data/CitedPapers\n",
    "! rm data/Fields"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e410f4dee0d5cace59cb3afedf8847a9cc2d0a50701e831213ee4ba8afe6822"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('DataScience': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
